FROM nvidia/cuda:12.4.1-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PATH="/venv/bin:$PATH"

# Install dependencies
RUN apt update && apt install -y \
    git \
    build-essential \
    cmake \
    ninja-build \
    curl \
    unzip \
    wget \
    libssl-dev \
    zlib1g-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    libffi-dev \
    libncursesw5-dev \
    xz-utils \
    tk-dev \
    libopenblas-dev \
    libomp-dev \
    libcurl4-openssl-dev \
    python3 \
    python3-pip \
    && ln -s /usr/bin/python3 /usr/bin/python \
    && rm -rf /var/lib/apt/lists/*

# Install Python 3.9
RUN cd /usr/src && \
    wget https://www.python.org/ftp/python/3.9.18/Python-3.9.18.tgz && \
    tar xzf Python-3.9.18.tgz && \
    cd Python-3.9.18 && \
    ./configure --enable-optimizations && \
    make -j$(nproc) && \
    make altinstall && \
    ln -s /usr/local/bin/python3.9 /usr/bin/python3.9 && \
    ln -s /usr/local/bin/pip3.9 /usr/bin/pip3.9 && \
    rm -rf /usr/src/Python-3.9.18*

# Create and initialize virtual environment
RUN python3.9 -m venv /venv && \
    /venv/bin/pip install --upgrade pip setuptools wheel

# Install Python packages inside venv
RUN /venv/bin/pip install \
    mutpy \
    beautifulsoup4 \
    pyyaml

# Clone and build llama.cpp with CUDA
RUN git clone https://github.com/ggerganov/llama.cpp.git /code/buildllama && \
    mkdir -p /code/buildllama/build && \
    cd /code/buildllama/build && \
    cmake .. -DGGML_CUDA=ON \
        -DGGML_CUBLAS=ON \
        -DCMAKE_CUDA_ARCHITECTURES=89 \
        -DCMAKE_LIBRARY_PATH=/usr/local/cuda/lib64/stubs \
        -DCMAKE_EXE_LINKER_FLAGS="-L/usr/local/cuda/lib64/stubs -lcuda" \
        -DCMAKE_SHARED_LINKER_FLAGS="-L/usr/local/cuda/lib64/stubs -lcuda" && \
    cmake --build . --config Release

# Set working directory and default command
WORKDIR /code

CMD ["/venv/bin/python", "main.py"]